---
title: "Airbnb Copenhagen"
author: "Studygroup 8"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---

```{r load_packages, warning=FALSE, message=FALSE, echo=FALSE}
list.of.packages <- c("ggplot2", "Rcpp", "tidyverse", "mosaic", "ggfortify", "moderndive", "janitor", "huxtable", "here", "broom", "skimr", "GGally", "car", "vroom")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
library(tidyverse)
library(mosaic)
library(ggfortify)
library(moderndive)
library(janitor)
library(huxtable)
library(here)
library(broom)
library(skimr)
library(GGally)
library(car)
library(vroom)
library(GGally)
library(leaflet)

```

#Introduction

Copenhagen, the happiest city in the world, Who doesn't want to go there! Happiness comes at a price though, you'll need to find a place to stay. What better way to look for a place to stay than on AirBnB and us being students, want to go there when the prices are cheapest. This is where the MAM program from LBS is already paying itself off. To find out what variables (e.g. room type, property type) influence the price most and to predict when is best to go, we're running a regression analysis. So our dream holiday can become reality.

The data we will be using for this comes directly from AirBnB. In this analysis we will only be using data on the city of Copenhagen. the data shows the listings as per June 26th 2020.

This analysis is divided into 4 parts:

1.    Executive summary
1.    Exploratory data analysis
1.    Regression analysis
1.    Conclusion

#Executive summary

to be filled in after conclusion.

#Exploratory data analysis
To get an idea of the data structure, an exploratory data analysis is conducted. The first steps are to look at the raw values and to clean the data afterwards. This gives a foundation to use in summary statistics. Finally, visualisations will give an impression of any correlations in variables and will give an outlook on variables that stand out.

##Looking at the raw values

First of all, the data needs to be loaded. The clean_names() function is used to make the values consistent and neat, to help with the data wrangling.

```{r cars, cache=TRUE}
listings <- vroom("http://data.insideairbnb.com/denmark/hovedstaden/copenhagen/2020-06-26/data/listings.csv.gz") %>%
    # Get the listings for Copenhagen
    clean_names()  
```

Having looked at the data, the following variables will be of importance throughout this analysis:

1.    price : cost per night

1.    cleaning_fee: cleaning fee

1.    extra_people: charge for having more than 1 person

1.    property_type: type of accommodation (House, Apartment, etc.)

1.    room_type:
      Entire home/apt (guests have entire place to themselves)
      Private room (Guests have private room to sleep, all other rooms shared)
      Shared room (Guests sleep in room shared with others)
      number_of_reviews: Total number of reviews for the listing

1.    review_scores_rating: Average review score (0 - 100)

1.    longitude , latitude: geographical coordinates to help us locate the listing

1.    neighborhood*: three variables on a few major neighborhoods in each city

*add any variables we are using to this list*


The next step is to check the data for the properties of the variables and if there are any missing or NA values that need to be addressed.

```{r, exploration, echo=FALSE, cache=TRUE}
<<<<<<< HEAD
# look at the raw values
glimpse(listings)

# compute summary statistics of the variables of interest, or find NAs
skim(listings) 
=======

#Get an overview of the size of the data and the variables
glimpse(listings) 


#Create an overview of the variables and their distribution, while checking for any missing or NA values
skim(listings)

>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
```

From this list can be concluded that there are 28,523 values for 106 variables. there are a couple of values that contain NA, 1 of them, cleaning_fee, stands out and is one of the variables that was flagged as important in this analysis. This will be addressed in the next section. Other variables that stand out are price, cleaning_fee and extra_people. these variables are stored as character values and need to have the $-sign removed to be stored as a numeric value. Since these variables will be used further in the analyis, this also will be resolved in the next section where we clean the data and run summary statistics.


##Computing summary statistics of the variables of interest


<<<<<<< HEAD
```{r cleaning data,echo=FALSE}
# Make price into a numeric variable
listings$price = as.numeric(gsub("[\\$,]", "", listings$price))

# Make cleaning fee into a numeric variable
listings$cleaning_fee = as.numeric(gsub("[\\$,]", "", listings$cleaning_fee))

# Make extra people a numeric variable
listings$extra_people = as.numeric(gsub("[\\$,]", "", listings$extra_people))

# Change NA cleaning fee values to 0, assuming that 0 is meant for NA
=======
In this section we will clean the data and run summary statistics that will be visualised in the next section. First the values stored as characters are converted to numeric values and the $-sign removed.

```{r cleaning data stored as character}

# make price into a numeric variable
listings$price = as.numeric(gsub("[\\$,]", "", listings$price)) 

# make cleaning fee into a numeric variable
listings$cleaning_fee = as.numeric(gsub("[\\$,]", "", listings$cleaning_fee)) 

# make extra people a numeric variable
listings$extra_people = as.numeric(gsub("[\\$,]", "", listings$extra_people))

```


Following this, the NA's in the cleaning_fee values will be converted to zero. Assuming the NA's are from hosts not providing an input for cleaning fees, the NA's should equal zero.

```{r cleaning data by reducing the NA's}

#Changes NA cleaning fee values to 0, because assuming that 0 is meant for NA
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
listings <- listings %>%   
  mutate(cleaning_fee = case_when(
    is.na(cleaning_fee) ~ 0, 
    TRUE ~ cleaning_fee
  ))

<<<<<<< HEAD
# Create a list of property types and the count
listings %>% 
  group_by(property_type) %>%
  summarise(count= n()) %>%
  arrange(desc(count))

# Take top 4 property_type and creat a new column and assigning property_type in groups
=======
```

The property type categorical variables has too many categories to be summarised. To reduce this, the 4 most substantial categories have been specified, the remainder will be classed as other.

```{r cleaning data by reducing property type criteria}

#create a list of property types and the count
listings%>% 
  group_by(property_type)%>%
  summarise(count= n())%>%
  arrange(desc(count))

#Taking top 4 property_type and creating a new column and assigning property_type in groups
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
listings <- listings %>%
  mutate(prop_type_simplified = case_when(
    property_type %in% c("Apartment","Condominium", "House","Townhouse") ~ property_type, 
    TRUE ~ "Other"
  ))

<<<<<<< HEAD
# Check if all the variables are in the correct type
skim(listings) 
=======
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
```

To only have a list of variables that will be used in the analysis, redundant variables are filtered out. As the analysis focuses on short holiday stays, the long stay listings need to be removed from the list. By looking at the minimum nights on the listings, it can be determined if these addresses are viable for short stays.

```{r filtering, echo=FALSE}
<<<<<<< HEAD
# Display table with the count in descending order for the minimum nights
listings %>%     
  group_by(minimum_nights) %>%
  summarise(count= n()) %>%
  arrange(desc(count))

# Filter the AirBnB data so that it only includes observations less than or equal to 4
=======

#displaying table with the count in descending order for the minimum nights
listings%>%     
  group_by(minimum_nights)%>%
  summarise(count= n())%>%
  arrange(desc(count))

```
In this summary it can be seen that after 5 minimum days the numbers of listing drop significantly.The majority of listings are within the 1 to 4 minimum days stay. Therefore this will give a good indication of the listings intended for short, holiday, stays. In the next section the data is filtered to only feature a minimum of 4 nights or less. Also, the variables that will be used in the analysis are selected, so to only have relevant variables left to make the analysis more concise and clear.

```{r filtering, echo=FALSE}
#Filtering the airBnB data so that it only includes observations less than or equal to 4
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
listings_filtered_4nights <- listings %>%
  filter(minimum_nights <= 4)

#Selecting the variables that will be used in the analysis as to remove the redundant variables.
listings_clean <- listings %>%
  select(id, 
         listing_url, 
         price, 
         guests_included, 
         cleaning_fee, 
         extra_people, 
         prop_type_simplified, 
         number_of_reviews, 
         review_scores_rating, 
         room_type, 
         bathrooms, 
         bedrooms, 
         beds, 
         accommodates, 
         host_is_superhost, 
         is_location_exact, 
         neighbourhood_cleansed, 
         cancellation_policy, 
         longitude, 
         latitude
        )

```

Let's have a look at the data.

```{r skim the new list}

#check if all the variables are in the correct type
skim(listings_clean) 

```


##Visualisations

Now that the data is clean, a selection of useful data has been made, and filtered to only feature our criteria for a short stay holiday listing, visualisations of data can be made. This lays the foundation of any correlations and questions the regression model will have to explain and answer.

First of all, we made a map with clusters of listings. This shows us where the listings in Copenhagen are and the concentration of listings. The reason we chose clusters instead of points, is that clusters made it easier to read and interpret.

```{r, echo=FALSE}
# Take the filtered listings with a min of 4 nights and create a map of the apartments in Copenhagen
listings_filtered_4nights %>% 
  leaflet() %>% 
  addProviderTiles("OpenStreetMap.Mapnik") %>% 
  addCircleMarkers(lng = ~longitude, 
                   lat = ~latitude, 
                   radius = 1,
                   fillOpacity = 0.4, 
                   popup = ~listing_url,
                   label = ~property_type,
                   #To get a better overview of the number and location of apartments in Copenhagen, a clustered map was chosen instead of the original point mapping
                   clusterOptions = markerClusterOptions()
                   )
  
```



```{r}
# Calculate median price per night for each property type
average_per_proptype <- listings_filtered_4nights %>% 
  group_by(prop_type_simplified) %>% 
  summarise(median_price = median(price))

# Plot a bar chart with median price per night for each property type
average_per_proptype %>% 
  ggplot(aes(
    x = reorder(prop_type_simplified, desc(median_price)),
    y = median_price
  )) +
  geom_col() +
  labs(
    title = "Median price per night per property type",
    x = "",
    y = "Median price per night",
    caption = "Source: AirBnB"
<<<<<<< HEAD
  )

# Calculate median price per night for each room type
=======
  ) +
 theme_tech(theme = "airbnb") +
  scale_color_tech(theme="airbnb")

#Calculated median price per night for each room type
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
median_per_roomtype <- listings_filtered_4nights %>% 
  group_by(room_type) %>% 
  summarise(median_price = median(price))

# Price per night for each room type
median_per_roomtype %>% 
  ggplot(aes(
    x = reorder(room_type, desc(median_price)),
    y = median_price
  )) +
  geom_col() +
  labs(
    title = "Median price per night per room type",
    x = "",
    y = "Median price per night",
    caption = "Source: AirBnB"
  )

<<<<<<< HEAD
# Display the correlation between four important variables
listings_matrix <- listings_filtered_4nights %>%
  select(price, square_feet, bedrooms, bathrooms) 
  ggpairs(listings_matrix, columns = 1:4)
=======
listings<-listings%>%
  filter(minimum_nights == 4)
listings_matrix<- listings %>%
  select(price, square_feet ,bedrooms, bathrooms) 
ggpairs(listings_matrix, columns = 1:4)

>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
```



```{r}
# Map price to seven colors using quantiles
qpal <- colorQuantile("Reds", listings_filtered_4nights$price, n = 7)

listings_filtered_4nights %>% 
  leaflet() %>% 
  addProviderTiles("OpenStreetMap.Mapnik") %>% 
  addCircleMarkers(lng = ~longitude, 
                   lat = ~latitude, 
                   radius = 1,
                   fillOpacity = 0.4, 
                   popup = ~listing_url,
                   label = ~property_type,
                   color = ~qpal(price))
```

# REGRESSION

## Basic explanatory variables  

```{r}
# Remove variables that will not be needed 
# listings_clean <- listings %>%
#   select(id, price, guests_included, cleaning_fee, extra_people, prop_type_simplified, number_of_reviews, review_scores_ratings, room_type, )
```


```{r}
# Create new variable for 4 nights using price, guests_included, cleaning_fee and extra_people
listings_new <- listings_filtered_4nights %>%
  mutate(price_4_nights = ifelse(
    guests_included <= 1,
    (price + extra_people) * 4 + cleaning_fee,
    (price) * 4 + cleaning_fee
  ))
```

```{r,fig.width=20}
# Density plots for price_4_nights
density.default(listings_new$price_4_nights)

# Calculate the most frequently occurring price (mode) for 4 nights 
max_p4n <- density(listings_new$price_4_nights)$x[which.max(density(listings_new$price_4_nights)$y)]
max_p4n

# Plot density of price_4_nights
ggplot(
  listings_new,
  aes(x = price_4_nights)) +
  geom_density() +
  # Plot vertical line to show maximum value
  geom_vline(xintercept =  max_p4n)  

# Plot density of log(price_4_nights)
ggplot(listings_new, aes(x = log(price_4_nights))) +
  geom_density()
```

Answer: we should use variable log(price_4_nights). Looking at the density plots of
price_4_nights and log(price_4_nights), we can see clearly that log(price_4_nights) is more close to a normal distribution, while most values in price_4_nights are cramped within a very small range.


### Model 0
```{r}

```


### Model 1

Fitting a regression model called model1 with the following explanatory variables: prop_type_simplified, number_of_reviews, and review_scores_rating

```{r model1}
# Create a new dataset with the new variable of log(price_4_nights)
listings_log <- listings_new %>%
  mutate(price_4_nights_log = log10(price_4_nights)) 
  
# Create model1 of price_4_nights with explanatory variables prop_type_simplified, number_of_reviews and review_scores_ratings
model1 <- lm(price_4_nights_log ~
              prop_type_simplified +
              number_of_reviews +
<<<<<<< HEAD
              review_scores_rating +
             NULL,
            data = listings_log
            )
=======
              review_scores_rating,
            data = listings_log)
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
model1 %>% tidy(conf.int=TRUE)
model1 %>% glance()

# Check residuals
autoplot(model1)
``` 

Interpretation of review_scores_rating predictor:

> Ratings (review_scores_rating) is a significant predictor. Controlling for other variables, every 1 point increase in rating is associated with $ 0.002 increase in log10(price_4_nights).

Interpretation of prop_type_simplified predictor:

> Property type (prop_type_simplified) is a significant predictor. Controlling for other variables, switching property type from apartment to condomunium would result in 0.034 increase in log10(price_4_nights). Similarly, switching property type from apartment to house would result in 0.11 increase in log10(price_4_nights); switching property type from apartment to townhouse would result in 0.17 increase in log10(price_4_nights); switching property type from apartment to property types other than the aforementioned 3 types would result in an average of 0.18 increase in log10(price_4_nights).


#### Model 1: 4%

### Model 2

We want to determine if room_type is a significant predictor of the cost for 4 nights, given everything else in the model.

```{r model2}
# Create model2 by adding room_type to model1
model2 <- lm(price_4_nights_log ~
             prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
<<<<<<< HEAD
             room_type +
             NULL, 
            data = listings_log
            )
model2 %>% tidy(conf.int=TRUE)
model2 %>% glance()

# Check residuals
=======
             room_type, 
            data = listings_log)
model2 %>% tidy(conf.int=TRUE)
model2 %>% glance()

# check residuals
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
autoplot(model2)
```

Interpretation of room_type predictor:

> The result of model2 regression shows that rooms_type is a significant predictor.
Controlling for other variables, switching room type from entire house to hotel room would result in 0.11 increase in log10(price_4_nights). Nevertheless, switching from entire house to private room would result in 0.27 drop in log10(price_4_nights); switching from entire house to shared room would result in 0.38 drop in log10(price_4_nights).

#### Model 2: 24% 


## Exploring additional explanatory variables 

### Model 3

Are the number of bathrooms, bedrooms, beds, or size of the house (accomodates) significant predictors of price_4_nights?

```{r model3a}
# Create model3a by adding bathrooms, bedrooms, beds, accommodates to model2 
model3a<-lm(price_4_nights_log ~
            prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
             room_type + 
             bathrooms +
             bedrooms +
             beds + 
<<<<<<< HEAD
             accommodates,
            data = listings_log
            )
=======
             accommodates, 
            data = listings_log)
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
model3a %>% 
  tidy(conf.int=TRUE)
model3a %>%
  glance()

# Test VIF
car::vif(model3a)

# Check residuals
autoplot(model3a)
```

Interpretation of bathrooms, bedrooms, beds, and size of the house (accommodates) predictors:

> The number of bathrooms, bedrooms and size of the house (accommodates) are  significant predictors of log10(price_4_nights) and are all positively related with price_4_nights, while number of beds is not a significant predictor. Coefficients show that each additional bathroom is associated with 0.06 increase in log10(price_4_nights); each additional bedroom is associated with 0.05 increase in log10(price_4_nights).
There might be a problem with multi-collinearity since intuitively the number of bathrooms and bedrooms should have a positive relationship with size of the house. A further analysis on VIF shows that size of the house has slight correlation with other predictors but is within an acceptable range.

```{r model3}
<<<<<<< HEAD
# Create model3 by removing prop_type from model3a 
model3 <- lm(price_4_nights_log ~
=======
# create model3 by removing beds from model3a 
model3<-lm(price_4_nights_log ~
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
             prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
             room_type + 
             bathrooms +
             bedrooms +
             accommodates, 
            data = listings_log
            )
model3 %>% tidy(conf.int=TRUE)
model3 %>% glance()

# Test VIF
car::vif(model3)

# Check residuals
autoplot(model3)
```

Model 3: 41%


### Model 4

Do superhosts command a pricing premium, after controlling for other variables?

```{r model4}
# Create model4 by adding host_is_superhost to model3
model4<-lm(price_4_nights_log ~
             prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
             room_type + 
             bathrooms +
             bedrooms +
             accommodates +
             host_is_superhost,
            data = listings_log
           )
model4 %>% tidy(conf.int=TRUE)
model4 %>% glance()

<<<<<<< HEAD
# Test VIF
=======
# test VIF
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
car::vif(model4)

# Check residuals
autoplot(model4)
```

Interpretation of superhost predictor:

> Since the p-value for the estimate of the superhost coefficient is  significant (with a p-value of 0.36 and a t-statistic of 0.9), it seems that superhosts do command a price premium from their guests in Copenhagen. Compared to non-superhost, being a superhost is associated with 0.03 increase in log10(price_4_nights).

```{r}
# Create model4b by adding host_is_superhost to model4 and create an interaction variable 
model4b<-lm(price_4_nights_log ~
              prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
             room_type + 
             bathrooms +
             bedrooms +
             accommodates +
             host_is_superhost*review_scores_rating,
            data = listings_log
            )
model4b %>% tidy(conf.int=TRUE)
model4b %>% glance()

<<<<<<< HEAD
# Test VIF
=======
# test VIF
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
car::vif(model4b)

# Check residuals
autoplot(model4b)
```

Interaction variable host_is_superhost*review_scores_rating is not significant, therefore we do not include it.



#### Model 4: 41% 


### Model 5

Most owners advertise the exact location of their listing (is_location_exact == TRUE), while a non-trivial proportion don’t. After controlling for other variables, is a listing’s exact location a significant predictor of price_4_nights?

```{r model5}
# Create model5 by adding is_location_exact == TRUE to model4
model5<-lm(price_4_nights_log ~
             prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
             room_type +
             bathrooms +
             bedrooms +
             accommodates +
             is_location_exact,
             data = listings_log
           )
model5 %>% tidy(conf.int=TRUE)
model5 %>% glance()

<<<<<<< HEAD
# Test VIF
=======
# test VIF
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
car::vif(model5)

# Check residuals
autoplot(model5)
```

Interpretation of exact location variable:

> A listing’s exact location variable has a p value of 0.34, and therefore is not a significant predictor of price_4_nights. Thus, we will not include it in the following models.


#### Model 4: 41% 



### Model 6

In order to reduce the number of neighbourhoods for our analysis we cluster the different neighbourhoods into groups, based on our experience, talking to locals, and research.

The city of Copenhagen has 10 official administrative districts. Additionally, there is Frederiksberg, which is an independent municipality and, thus, separate from the Copenhagen Municipality, however, it is still part of Copenhagen city. 

Indre By is the city center of Copenhagen and will remain its own cluster "Center".

All the residential neighbourhoods surrounding the city center are commonly grouped together, and since they all end in -bro, often referred to as -bro districts (Brokvaterer in Danish). Frederiksberg is often also included in the -bro districts. Hence, we will cluster these 4 Brokvaterer districts into one group of Brokvarterer 

Additionally, the two Amager districts (Vest and Ost) are grouped together.

In the West, Valby, Vanlose and Brønshøj-Husum are summarized.

Lastly, Bispebjerg, often referred to as Nordvest, which is more of a residential neighbourhood, is its own cluster.
```{r model6}
# Create variable neighbourhood_simplified with 5 categories 
listings_log <- listings_log %>%
  mutate(neighbourhood_simplified = case_when(
    neighbourhood_cleansed %in% c("Indre By") ~ "Center",
    neighbourhood_cleansed %in% c("Frederiksberg","Nrrebro", "sterbro","Vesterbro-Kongens Enghave") ~ "Brokvarterer", 
    neighbourhood_cleansed %in% c("Amager st","Amager Vest") ~ "Amager",
    neighbourhood_cleansed %in% c("Brnshj-Husum","Valby", "Vanlose") ~ "West",
    TRUE ~ "Nordvest"
  ))

# Create model6 by adding neighbourhood_simplified to model3
model6<-lm(price_4_nights_log ~
            prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
             room_type +
             bathrooms +
             bedrooms +
             accommodates +
             neighbourhood_simplified +
             NULL,
             data = listings_log
           )
model6 %>% tidy(conf.int=TRUE)
model6 %>% glance()

<<<<<<< HEAD
# Test VIF
=======
# test VIF
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
car::vif(model6)

# Check residuals
autoplot(model6)
```

One can see that the neighbourhood of an apartment is an predictor of the price for 4 nights in Copenhagen.

Interpretation of neighbourhood predictor:

> We can see that all the neighbourhood variables are significant predictors of the price for 4 nights in Copenhagen. Taking Amager as the base, Brokvarterer is asscociated with 0.03 more in log10(price_4_nights); Center is asscociated with 0.16 more in log10(price_4_nights); Nordvest is asscociated with 0.09 drop in log10(price_4_nights); West is asscociated with 0.09 drop in log10(price_4_nights).

#### Model 6: 50%


### Model 7

What is the effect of cancellation_policy on price_4_nights, after we control for other variables?

```{r model7}
# Create model7 by adding cancellation_policy to model6
model7<-lm(price_4_nights_log ~
            prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
             room_type +
             bathrooms +
             bedrooms +
             accommodates +
             neighbourhood_simplified +
             cancellation_policy,
             data = listings_log
           )
model7 %>% tidy(conf.int=TRUE)
model7 %>% glance()

<<<<<<< HEAD
# Test VIF
=======
# test VIF
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
car::vif(model7)

# Check residuals
autoplot(model7)
```

The cancellation policy is a significant predictor on price, with a flexible policy commanding the lowest price and with a strict policy commanding the highest price. This is surprising since usually hosts should be rewarded for the risk that they are taking with a flexibly cancellation policy. On the flipside, usually expensive and nice places have a stricter cancellation policy since they cannot afford to not have anyone live in their AirBnB. Thus, in the end the cancellation policy might rather be a consequence of the price and not a predictor of it. 



Interpretation of cancellation policy predictor:

> The cancellation policy is a significant predictor on price, with a flexible policy commanding the lowest price and with a strict policy commanding the highest price. This is surprising since usually hosts should be rewarded for the risk that they are taking with a flexible cancellation policy. On the other hand, usually expensive and nice places have a stricter cancellation policy since the cost of having the place empty for a night is relatively higher. Thus, in the end the cancellation policy might rather be a consequence of the price and not a predictor of it. 

#### Model 7: 51%


# PREDICTION

## Selecting the best model 

After creating all these models, we now find to find our best model, our final model for predictions.

```{r hux-copenhagen, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
# Compare all models next to each other
huxreg(model1, model2, model3, model4, model5, model6, model7,
       statistics = c('#observations' = 'nobs', 
                      'R squared' = 'r.squared', 
                      'Adj. R Squared' = 'adj.r.squared', 
                      'Residual SE' = 'sigma'
                      ), 
       bold_signif = 0.05, 
       stars = NULL
) %>% 
  set_caption('Comparison of models')
```

Model 7 is the best model with the highest R2 so this will be the final model.


## Final model 
```{r final model}
final_model <- lm(price_4_nights_log ~
             prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
             room_type +
             bathrooms +
             bedrooms +
             accommodates +
             neighbourhood_simplified +
<<<<<<< HEAD
             cancellation_policy +
             NULL,
             data = listings_log
             )

=======
             cancellation_policy,
             data = listings_log)
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
mosaic::msummary(final_model)
get_regression_table(final_model)
get_regression_summaries(final_model)
vif(final_model)
```

## Prediction

We are planning to visit Copenhagen and want to stay in an Airbnb. We would like to look at Airbnbs that are apartments with a private room, have at least 10 reviews, and an average rating of at least 90. 

We will predict the total cost to stay at this Airbnb for 4 nights, including the appropriate 95% interval with the prediction. (Report the point prediction and interval in terms of price_4_nights)

```{r predict-price, warning=FALSE}
# Filter our the data by room_type, nr of reviews and rating
listings_predict_log <- listings_log %>% 
  filter(room_type == "Private room",
         number_of_reviews >= 10,
         review_scores_rating >= 90
         ) 

listings_predict_log

<<<<<<< HEAD
# # Generate a prediction for each row
# predict(final_model, newdata = listings_predict_log, interval = "prediction")
# 
# # Generate a prediction for each row with broom 
# model_predictions_log <- broom::augment(final_model, 
#                              newdata = listings_predict_log)
# 
# 
# 
# model_predictions

# Augment the model
model_final_augment <- augment(final_model)
=======
# generate a prediction for each row
model_predictions_log_CI <- predict(final_model, newdata = listings_predict_log, interval = "confidence")
model_predictions_log_CI

# generate a prediction for each row with broom
model_predictions_log <- broom::augment(final_model,
                             newdata = listings_predict_log, se_fit = TRUE)
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde

model_predictions_log

# model_predictions_aug <- augment(final_model)
# 
# model_predictions_aug

<<<<<<< HEAD
# Convert prices back from log
model_predictions <- model_final_augment %>%
  mutate(fitted_price_final = 10^(.fitted))
=======
# convert prices back from log
model_predictions <- model_predictions_log  %>%
  mutate(lower = .fitted - 1.96 * .se.fit, 
         upper = .fitted + 1.96 * .se.fit) %>%
  mutate(fitted_price_final = 10^(.fitted),
         upper_final = 10^(upper),
         lower_final = 10^(lower))
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde

model_predictions
```

```{r graph-predictions, warning=FALSE}
<<<<<<< HEAD
# Graph distribution of fitted values and converting the log values back to normal dollar prices 
=======
# change order of property types to have other in the end
model_predictions$prop_type_simplified <- factor(model_predictions$prop_type_simplified, c("Apartment", "Condominium", "House", "Townhouse", "Other"))

# graph distribution of fitted values  
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
ggplot(model_predictions, aes(x = fitted_price_final)) +
  geom_density() +
  labs(y = "",
       x = "Price of an AirBnB in Copenhagen") +
  theme_bw()

<<<<<<< HEAD
# Facet by neighbourhood
=======
# facet by neighbourhood
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
ggplot(model_predictions, aes(x = fitted_price_final)) +
  geom_density() +
  labs(y = "",
       x = "Price of an AirBnB in Copenhagen") +
  facet_wrap(~neighbourhood_simplified) +
  theme_bw()
<<<<<<< HEAD

# Facet by prop_type_simplified
=======
  
# facet by prop_type_simplified
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
ggplot(model_predictions, aes(x = fitted_price_final)) +
  geom_density() +
  labs(y = "",
       x = "Price of an AirBnB in Copenhagen") +
  facet_wrap(~prop_type_simplified) +
  theme_bw()

<<<<<<< HEAD
# Facet by neighbourhood & prop_type_simplified
=======
# facet by neighbourhood & prop_type_simplified
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
ggplot(model_predictions, aes(x = fitted_price_final)) +
  geom_density() +
  labs(y = "",
       x = "Price of an AirBnB in Copenhagen") +
  facet_grid(neighbourhood_simplified ~ prop_type_simplified) +
  theme_bw()
```

```{r}
# Find mean and median of the fitted prices
model_predictions %>%
  summarize(mean_price = mean(fitted_price_final, na.rm=TRUE), median_price = median(fitted_price_final, na.rm=TRUE))
```

> As the price distribution is **right skewed**, choosing mean as measure to predict the final price wouldn't be economical so we've decided to use the median because it fits our budget.

```{r}
<<<<<<< HEAD
# Locate the median value property
predicted_value<-listings_predict_log %>%
  filter(fitted_price_final == median(fitted_price_final))
=======
# locate the median value property
predicted_value <- model_predictions %>%
  filter(fitted_price_final == median(fitted_price_final, na.rm=TRUE))
>>>>>>> a5ab433266bf3151f09fccfb23f7d38de0d6ffde
predicted_value


print(predicted_value$lower_final) + print(predicted_value$fitted_price_final) + print(predicted_value$upper_final)

```

predicted price: 1887.023 $
confidence interval:	$ 1839.692 $ 1935.572



To DO
- select relevant variables
- interaction variables
- write out explanations & analyses (e.g. for residual plots)
- add titles to graphs
