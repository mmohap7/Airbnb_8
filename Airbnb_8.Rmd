---
title: "Airbnb_8"
output: html_document
---

```{r load_packages, warning=FALSE, message=FALSE, echo=FALSE}
list.of.packages <- c("ggplot2", "Rcpp", "tidyverse", "mosaic", "ggfortify", "moderndive", "janitor", "huxtable", "here", "broom", "skimr", "GGally", "car", "vroom")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
library(tidyverse)
library(mosaic)
library(ggfortify)
library(moderndive)
library(janitor)
library(huxtable)
library(here)
library(broom)
library(skimr)
library(GGally)
library(car)
library(vroom)
library(GGally)
library(leaflet)
```

#Introduction

Copenhagen, the happiest city in the world, Who doesn't want to go there! Happiness comes at a price though, you'll need to find a place to stay. What better way to look for a place to stay than on AirBnB and us being students, want to go there when the prices are cheapest. This is where the MAM program from LBS is already paying itself off. To find out what variables (e.g. room type, property type) influence the price most and to predict when is best to go, we're running a regression analysis. So our dream holiday can become reality.

The data we will be using for this comes directly from AirBnB. In this analysis we will only be using data on the city of Copenhagen. the data shows the listings as per June 26th 2020.

This analysis is divided into 4 parts:

1.    Executive summary
1.    Exploratory data analysis
1.    Regression analysis
1.    Conclusion

#Executive summary

to be filled in after conclusion.

#Exploratory data analysis
To get an idea of the data structure, an exploratory data analysis is conducted. The first steps are to look at the raw values and to clean the data afterwards. This gives a foundation to use in summary statistics. Finally, visualisations will give an impression of any correlations in variables and will give an outlook on variables that stand out.

##Looking at the raw values

First of all, the data needs to be loaded. The clean_names() function is used to make the values consistent and neat, to help with the data wrangling.

```{r cars, cache=TRUE}
listings <- vroom("http://data.insideairbnb.com/denmark/hovedstaden/copenhagen/2020-06-26/data/listings.csv.gz") %>% 
    #get the listings for Copenhagen
    clean_names()  
```

Throughout the analysis the following variables will be of importance
price = cost per night

cleaning_fee: cleaning fee

extra_people: charge for having more than 1 person

property_type: type of accommodation (House, Apartment, etc.)

room_type:

Entire home/apt (guests have entire place to themselves)
Private room (Guests have private room to sleep, all other rooms shared)
Shared room (Guests sleep in room shared with others)
number_of_reviews: Total number of reviews for the listing

review_scores_rating: Average review score (0 - 100)

longitude , latitude: geographical coordinates to help us locate the listing

neighbourhood*: three variables on a few major neighbourhoods in each city

```{r, exploration, echo=FALSE, cache=TRUE}
# Looking at the raw values
glimpse(listings) 
#Computing summary statistics of the variables of interest, or finding NAs
skim(listings) 
```



##Computing summary statistics of the variables of interest

We tried first to convert some of the data from characters to numeric (i.e “$176.00”--. 176)

```{r cleaning data,echo=FALSE}
# make price into a numeric variable
listings$price = as.numeric(gsub("[\\$,]", "", listings$price)) 
# make cleaning fee into a numeric variable
listings$cleaning_fee = as.numeric(gsub("[\\$,]", "", listings$cleaning_fee)) 
# make extra people a numeric variable
listings$extra_people = as.numeric(gsub("[\\$,]", "", listings$extra_people)) 
#Changes NA cleaning fee values to 0, because assuming that 0 is meant for NA
listings <- listings %>%   
  mutate(cleaning_fee = case_when(
    is.na(cleaning_fee) ~ 0, 
    TRUE ~ cleaning_fee
  ))
#create a list of property types and the count
listings%>% 
  group_by(property_type)%>%
  summarise(count= n())%>%
  arrange(desc(count))
#Taking top 4 property_type and creating a new column and assigning property_type in groups
listings <- listings %>%
  mutate(prop_type_simplified = case_when(
    property_type %in% c("Apartment","Condominium", "House","Townhouse") ~ property_type, 
    TRUE ~ "Other"
  ))
#check if all the variables are in the correct type
skim(listings) 
```
```{r filtering, echo=FALSE}
#displaying table with the count in descending order for the minimum nights
listings%>%     
  group_by(minimum_nights)%>%
  summarise(count= n())%>%
  arrange(desc(count))
#Filtering the airbnb data so that it only includes observations less than or equal to 4
listings_filtered_4nights <- listings %>%
  filter(minimum_nights <= 4)
```
> The most common values are 2,3,1,4,5.That means renting for a five days or less.
We see that there is a dramatic decrease of the number of listing from 5 days and above and this may be due to the fact that people staying for lomger periods are not here for travel purposes but for other reasons such as work, living etc.

```{r, echo=FALSE}
#Taking the filtered listings with a min of 4 nights and create a map of the apartments in Copenhagen
listings_filtered_4nights %>% 
leaflet() %>% 
  addProviderTiles("OpenStreetMap.Mapnik") %>% 
  addCircleMarkers(lng = ~longitude, 
                   lat = ~latitude, 
                   radius = 1,
                   fillOpacity = 0.4, 
                   popup = ~listing_url,
                   label = ~property_type,
                   #to get a better overview of the number and location of apartments in Copenhagen, a clustered map was chosen instead of the original point mapping.
                   clusterOptions = markerClusterOptions()
                  )
  
```
```{r}
#Calculated median price per night for each property type
average_per_proptype <- listings_filtered_4nights %>% 
  group_by(prop_type_simplified) %>% 
  summarise(median_price = median(price))
#Plotted a bar chart with median price per night for each property type
average_per_proptype %>% 
  ggplot(aes(
    x = reorder(prop_type_simplified, desc(median_price)),
    y = median_price
  )) +
  geom_col() +
  labs(
    title = "Median price per night per property type",
    x = "",
    y = "Median price per night",
    caption = "Source: AirBnB"
  )
#Calculated median price per night for each room type
median_per_roomtype <- listings_filtered_4nights %>% 
  group_by(room_type) %>% 
  summarise(median_price = median(price))
#Price per night for each room type
median_per_roomtype %>% 
  ggplot(aes(
    x = reorder(room_type, desc(median_price)),
    y = median_price
  )) +
  geom_col() +
  labs(
    title = "Median price per night per room type",
    x = "",
    y = "Median price per night",
    caption = "Source: AirBnB"
  )
listings<-listings%>%
  filter(minimum_nights == 4)
listings_matrix<- listings %>%
  select(price, square_feet ,bedrooms, bathrooms) 
ggpairs(listings_matrix, columns = 1:4)
```



```{r}
#maps price to seven colors using quantiles
qpal <- colorQuantile("Reds", listings_filtered_4nights$price, n = 7)

listings_filtered_4nights %>% 
leaflet() %>% 
  addProviderTiles("OpenStreetMap.Mapnik") %>% 
  addCircleMarkers(lng = ~longitude, 
                   lat = ~latitude, 
                   radius = 1,
                   fillOpacity = 0.4, 
                   popup = ~listing_url,
                   label = ~property_type,
                   color = ~qpal(price))
```

# REGRESSION

## Basic explanatory variables  

```{r}
# remove variables that will not be needed 
# listings_clean <- listings %>%
#   select(id, price, guests_included, cleaning_fee, extra_people, prop_type_simplified, number_of_reviews, review_scores_ratings, room_type, )
```


```{r}
# create new variable for 4 nights using price, guests_included, cleaning_fee and extra_people
listings_new <- listings_filtered_4nights %>%
  mutate(price_4_nights = 
           ifelse(guests_included <= 1,
                  (price + extra_people) * 4 + cleaning_fee,
                  (price) * 4 + cleaning_fee))
```

```{r,fig.width=20}
# density plots for price_4_nights
density.default(listings_new$price_4_nights)

# calculate the most frequently occurring price (mode) for 4 nights 
max_p4n<-density(listings_new$price_4_nights)$x[which.max(density(listings_new$price_4_nights)$y)]
max_p4n

# plot density of price_4_nights
ggplot(
  listings_new,
  aes(x=price_4_nights))+
  geom_density() +
  # plot vertical line to show maximum value
  geom_vline(xintercept =  max_p4n)  

# plot density of log(price_4_nights)
ggplot(listings_new,aes(x=log(price_4_nights)))+
  geom_density()
```

Answer: we should use variable log(price_4_nights). Looking at the density plots of
price_4_nights and log(price_4_nights), we can see clearly that log(price_4_nights) is more close to a normal distribution, while most values in price_4_nights are cramped within a very small range.


### Model 1

Fitting a regression model called model1 with the following explanatory variables: prop_type_simplified, number_of_reviews, and review_scores_rating

```{r model1}
# create a new dataset with the new variable of log(price_4_nights)
listings_log <- listings_new %>%
  mutate(price_4_nights_log = log10(price_4_nights)) 
  
# create model1 of price_4_nights with explanatory variables prop_type_simplified, number_of_reviews and review_scores_ratings
model1<-lm(price_4_nights_log ~
              prop_type_simplified +
              number_of_reviews +
              review_scores_rating +
             NULL,
            data = listings_log)
model1 %>%
  tidy(conf.int=TRUE)
model1 %>%
  glance()
# check residuals
autoplot(model1)
``` 

Interpretation of review_scores_rating predictor:

> Ratings (review_scores_rating) is a significant predictor. Controlling for other variables, every 1 point increase in rating is associated with $ 0.002 increase in log10(price_4_nights).

Interpretation of prop_type_simplified predictor:

> Property type (prop_type_simplified) is a significant predictor. Controlling for other variables, switching property type from apartment to condomunium would result in 0.034 increase in log10(price_4_nights). Similarly, switching property type from apartment to house would result in 0.11 increase in log10(price_4_nights); switching property type from apartment to townhouse would result in 0.17 increase in log10(price_4_nights); switching property type from apartment to property types other than the aforementioned 3 types would result in an average of 0.18 increase in log10(price_4_nights)


#### Model 1: 4%

### Model 2

We want to determine if room_type is a significant predictor of the cost for 4 nights, given everything else in the model.

```{r model2}
# create model2 by adding room_type to model1
model2<-lm(price_4_nights_log ~
             prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
             room_type +
             NULL, 
            data = listings_log)
model2 %>% tidy(conf.int=TRUE)
model2 %>% glance()
# check residuals
autoplot(model2)
```

Interpretation of room_type predictor:

> The result of model2 regression shows that rooms_type is a significant predictor.
Controlling for other variables, switching room type from entire house to hotel room would result in 0.11 increase in log10(price_4_nights). Nevertheless, switching from entire house to private room would result in 0.27 drop in log10(price_4_nights); switching from entire house to shared room would result in 0.38 drop in log10(price_4_nights).

#### Model 2: 24% 


## Exploring additional explanatory variables 

### Model 3

Are the number of bathrooms, bedrooms, beds, or size of the house (accomodates) significant predictors of price_4_nights?

```{r model3a}
# create model3a by adding bathrooms, bedrooms, beds, accommodates to model2 
model3a<-lm(price_4_nights_log ~
            prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
             room_type + 
             bathrooms +
             bedrooms +
             beds + 
             accommodates,
            data = listings_log)
model3a %>% 
  tidy(conf.int=TRUE)
model3a %>%
  glance()
# test VIF
car::vif(model3a)
# check residuals
autoplot(model3a)
```

Interpretation of bathrooms, bedrooms, beds, and size of the house (accommodates) predictors:

> The number of bathrooms, bedrooms and size of the house (accommodates) are  significant predictors of log10(price_4_nights) and are all positively related with price_4_nights, while number of beds is not a significant predictor. Coefficients show that each additional bathroom is associated with 0.06 increase in log10(price_4_nights); each additional bedroom is associated with 0.05 increase in log10(price_4_nights).
There might be a problem with multi-collinearity since intuitively the number of bathrooms and bedrooms should have a positive relationship with size of the house. A further analysis on VIF shows that size of the house has slight correlation with other predictors but is within an acceptable range.

????????However, the prop_type becomes not a significant predictor anymore, thus we will include it from the following analyses:

```{r model3}
# create model3 by removing prop_type from model3a 
model3<-lm(price_4_nights_log ~
             prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
             room_type + 
             bathrooms +
             bedrooms +
             accommodates, 
            data = listings_log)
model3 %>% 
  tidy(conf.int=TRUE)
model3 %>% 
  glance()
# test VIF
car::vif(model3)
# check residuals
autoplot(model3)
```

Model 3: 41%


### Model 4

Do superhosts command a pricing premium, after controlling for other variables?

```{r model4}
# create model4 by adding host_is_superhost to model3
model4<-lm(price_4_nights_log ~
             prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
             room_type + 
             bathrooms +
             bedrooms +
             accommodates +
             host_is_superhost,
            data = listings_log)
model4 %>% tidy(conf.int=TRUE)
model4 %>% glance()
# test VIF
car::vif(model4)
# check residuals
autoplot(model4)
```

Interpretation of superhost predictor:

> Since the p-value for the estimate of the superhost coefficient is  significant (with a p-value of 0.36 and a t-statistic of 0.9), it seems that superhosts do command a price premium from their guests in Copenhagen. Compared to non-superhost, being a superhost is associated with 0.03 increase in log10(price_4_nights).

```{r}
# create model4b by adding host_is_superhost to model4 and create an interaction variable 
model4b<-lm(price_4_nights_log ~
              prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
             room_type + 
             bathrooms +
             bedrooms +
             accommodates +
             host_is_superhost*review_scores_rating,
            data = listings_log)
model4b %>% tidy(conf.int=TRUE)
model4b %>% glance()
# test VIF
car::vif(model4b)
# check residuals
autoplot(model4b)
```

Interaction variable host_is_superhost*review_scores_rating is not significant, therefore we do not include it.


#### Model 4: 41% 


### Model 5

Most owners advertise the exact location of their listing (is_location_exact == TRUE), while a non-trivial proportion don’t. After controlling for other variables, is a listing’s exact location a significant predictor of price_4_nights?

```{r model5}
# create model5 by adding is_location_exact == TRUE to model3
model5<-lm(price_4_nights_log ~
             prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
             room_type +
             bathrooms +
             bedrooms +
             accommodates +
             is_location_exact,
             data = listings_log)
model5 %>% tidy(conf.int=TRUE)
model5 %>% glance()
# test VIF
car::vif(model5)
# check residuals
autoplot(model5)
```

Interpretation of exact location variable:

> A listing’s exact location variable has a p value of 0.34, and therefore is not a significant predictor of price_4_nights. Thus, we will not include it in the following models.


### Model 6

In order to reduce the number of neighbourhoods for our analysis we cluster the different neighbourhoods into groups, based on our experience, talking to locals, and research.

The city of Copenhagen has 10 official administrative districts. Additionally, there is Frederiksberg, which is an independent municipality and, thus, separate from the Copenhagen Municipality, however, it is still part of Copenhagen city. 

Indre By is the city center of Copenhagen and will remain its own cluster "Center".

All the residential neighbourhoods surrounding the city center are commonly grouped together, and since they all end in -bro, often referred to as -bro districts (Brokvaterer in Danish). Frederiksberg is often also included in the -bro districts. Hence, we will cluster these 4 Brokvaterer districts into one group of Brokvarterer 

Additionally, the two Amager districts (Vest and Ost) are grouped together.

In the West, Valby, Vanlose and Brønshøj-Husum are summarized.

Lastly, Bispebjerg, often referred to as Nordvest, which is more of a residential neighbourhood, is its own cluster.
```{r model6}
# create variable neighbourhood_simplified with 5 categories 
listings_log <- listings_log %>%
  mutate(neighbourhood_simplified = case_when(
    neighbourhood_cleansed %in% c("Indre By") ~ "Center",
    neighbourhood_cleansed %in% c("Frederiksberg","Nrrebro", "sterbro","Vesterbro-Kongens Enghave") ~ "Brokvarterer", 
    neighbourhood_cleansed %in% c("Amager st","Amager Vest") ~ "Amager",
    neighbourhood_cleansed %in% c("Brnshj-Husum","Valby", "Vanlose") ~ "West",
    TRUE ~ "Nordvest"
  ))
# create model6 by adding neighbourhood_simplified to model3
model6<-lm(price_4_nights_log ~
            prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
             room_type +
             bathrooms +
             bedrooms +
             accommodates +
             neighbourhood_simplified +
             NULL,
             data = listings_log)
model6 %>% tidy(conf.int=TRUE)
model6 %>% glance()
# test VIF
car::vif(model6)
# check residuals
autoplot(model6)
```

Interpretation of neighbourhood predictor:

> We can see that all the neighbourhood variables are significant predictors of the price for 4 nights in Copenhagen. Taking Amager as the base, Brokvarterer is asscociated with 0.03 more in log10(price_4_nights); Center is asscociated with 0.16 more in log10(price_4_nights); Nordvest is asscociated with 0.09 drop in log10(price_4_nights); West is asscociated with 0.09 drop in log10(price_4_nights).


#### Model 6: 50%


### Model 7

What is the effect of cancellation_policy on price_4_nights, after we control for other variables?
```{r model7}
# create model7 by adding cancellation_policy to model6
model7<-lm(price_4_nights_log ~
            prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
             room_type +
             bathrooms +
             bedrooms +
             accommodates +
             neighbourhood_simplified +
             cancellation_policy,
             data = listings_log)
model7 %>% tidy(conf.int=TRUE)
model7 %>% glance()
# test VIF
car::vif(model7)
# check residuals
autoplot(model7)
```

Interpretation of cancellation policy predictor:

> The cancellation policy is a significant predictor on price, with a flexible policy commanding the lowest price and with a strict policy commanding the highest price. This is surprising since usually hosts should be rewarded for the risk that they are taking with a flexible cancellation policy. On the other hand, usually expensive and nice places have a stricter cancellation policy since the cost of having the place empty for a night is relatively higher. Thus, in the end the cancellation policy might rather be a consequence of the price and not a predictor of it. 



# PREDICTION

## Selecting the best model 

After creating all these models, we now find to find our best model, our final model for predictions.

```{r hux-copenhagen, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
# compare all models next to each other
huxreg(model1, model2, model3, model4, model5, model6, model7,
       statistics = c('#observations' = 'nobs', 
                      'R squared' = 'r.squared', 
                      'Adj. R Squared' = 'adj.r.squared', 
                      'Residual SE' = 'sigma'), 
       bold_signif = 0.05, 
       stars = NULL
) %>% 
  set_caption('Comparison of models')
```

Model 7 is the best model with the highest R2 so this will be the final model.


## Final model 
```{r final model}
final_model <- lm(price_4_nights_log ~
            prop_type_simplified +
             number_of_reviews +
             review_scores_rating +
             room_type +
             bathrooms +
             bedrooms +
             accommodates +
             neighbourhood_simplified +
             cancellation_policy +
             NULL,
             data = listings_log)
mosaic::msummary(final_model)
get_regression_table(final_model)
get_regression_summaries(final_model)
vif(final_model)
```

## Prediction

We are planning to visit Copenhagen and want to stay in an Airbnb. We would like to look at Airbnbs that are apartments with a private room, have at least 10 reviews, and an average rating of at least 90. 

We will predict the total cost to stay at this Airbnb for 4 nights, including the appropriate 95% interval with the prediction. (Report the point prediction and interval in terms of price_4_nights)

```{r predict-price, warning=FALSE}
# filter our the data by room_type, nr of reviews and rating
listings_predict <- listings_log %>% 
  mutate(price_final = 10^(price_4_nights_log)) %>%
  filter(room_type == "Private room",
         number_of_reviews >= 10,
         review_scores_rating >= 90,
         prop_type_simplified %in% c("Apartment", "Condominium", "House")) # there are no values for Townhouse or Other after filtering for our conditions, so we exclude the from the dataset
# When we plug this multi-row data frame into predict(), it'll generate a
# prediction for each row
predict(final_model, newdata = listings_predict, interval = "prediction")
# We can also use broom::augment(). It's  essentially the same thing as predict(), 
# but it adds the predictions and confidence intervals to the imaginary constituency 
model_predictions <- broom::augment(final_model, 
                             newdata = listings_predict)
# graph distribution of fitted values and converting the log values back to normal dollar prices 
ggplot(model_predictions, aes(x = 10^(.fitted))) +
  geom_density() +
  labs(y = "Predicted leave share",
       x = "Proportion of constituency with a university degree") +
  theme_bw()
# facet by neighbourhood
ggplot(model_predictions, aes(x = 10^(.fitted))) +
  geom_density() +
  labs(y = "Predicted leave share",
       x = "Proportion of constituency with a university degree") +
  facet_wrap(~neighbourhood_simplified) +
  theme_bw()
# facet by prop_type_simplified
ggplot(model_predictions, aes(x = 10^(.fitted))) +
  geom_density() +
  labs(y = "Predicted leave share",
       x = "Proportion of constituency with a university degree") +
  facet_wrap(~prop_type_simplified) +
  theme_bw()
# facet by neighbourhood & prop_type_simplified
ggplot(model_predictions, aes(x = 10^(.fitted))) +
  geom_density() +
  labs(y = "Predicted leave share",
       x = "Proportion of constituency with a university degree") +
  facet_grid(neighbourhood_simplified~prop_type_simplified) +
  theme_bw()
```
```{r}
# find mean and median of the fitted prices
model_predictions %>%
  summarize(mean_price = mean(price_final), median_price = median(price_final))
```

> As the price distribution is **right skewed**, choosing mean as measure to predict the final price wouldn't be economical so we've decided to use the median because it fits our budget.
```{r}
# locate the median value property
predicted_value<-listings_predict %>%
  filter(price_final == median(price_final))
predicted_value
```



To DO
- select relevant variables
- interaction variables
- write out explanations & analyses
